{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArunGNair06/123/blob/main/IMDLIB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfjD2mg2vqEa"
      },
      "outputs": [],
      "source": [
        "# Connect Google drive with Google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install imdlib # Cell 2: install dependencies (run once)\n",
        "pip install imdlib geopandas rioxarray xarray rasterio pyproj shapely\n",
        "# rioxarray installs rasterio/xarray and makes reprojection easier\n"
      ],
      "metadata": {
        "id": "PtPYYBK4vujK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: main script - download IMD data, clip to AOI, export CSV\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import imdlib as imd\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "\n",
        "# ----------------- USER CONFIG -----------------\n",
        "# path on your Google Drive where data will be saved / read\n",
        "drive_base = \"/drive/MyDrive/GEE_Export/Rainfall\"\n",
        "\n",
        "# IMD time range & variable\n",
        "start_yr = 2019\n",
        "end_yr = 2020\n",
        "variable = \"rain\"   # 'rain', 'tmin', 'tmax' (as supported by imdlib)\n",
        "\n",
        "# your AOI shapefile path (on Drive). It can be a single polygon or multi-polygon.\n",
        "shapefile_path = \"/drive/MyDrive/GEE_Exports_NDVI_LST/KochiCorp_shapefile.shp\"\n",
        "\n",
        "# output CSV path (combined)\n",
        "out_csv = os.path.join(drive_base, f\"imd_{variable}_{start_yr}_{end_yr}_points.csv\")\n",
        "\n",
        "# If you prefer separate CSV per polygon (if shapefile has multiple features), set True:\n",
        "save_one_csv_per_polygon = False\n",
        "one_csv_folder = os.path.join(drive_base, \"per_polygon_csvs\")\n",
        "os.makedirs(one_csv_folder, exist_ok=True)\n",
        "\n",
        "# ------------------------------------------------\n",
        "\n",
        "os.makedirs(drive_base, exist_ok=True)\n",
        "\n",
        "print(\"Downloading IMD data (this may take a while the first time)...\")\n",
        "# This will download yearwise files into drive_base (folder)\n",
        "imd.get_data(variable, start_yr, end_yr, fn_format=\"yearwise\", file_dir=drive_base)\n",
        "\n",
        "print(\"Opening downloaded data...\")\n",
        "# open_data returns an object from imdlib; get_xarray gives xarray.Dataset\n",
        "data_obj = imd.open_data(variable, start_yr, end_yr, \"yearwise\", drive_base)\n",
        "ds = data_obj.get_xarray()   # xarray Dataset\n",
        "\n",
        "# pick the main data variable inside xarray dataset\n",
        "# usually ds[variable] exists; if not, pick the first data variable\n",
        "if variable in ds.data_vars:\n",
        "    da = ds[variable]\n",
        "else:\n",
        "    # fallback: pick first data variable\n",
        "    first_var = list(ds.data_vars.keys())[0]\n",
        "    print(f\"Warning: variable '{variable}' not found in dataset. Using '{first_var}' instead.\")\n",
        "    da = ds[first_var]\n",
        "\n",
        "print(\"Dataset dims:\", ds.dims)\n",
        "print(\"Data variable dims:\", da.dims)\n",
        "print(da)\n",
        "# Ensure lat/lon dims exist; common names: 'lat' and 'lon' or 'latitude'/'longitude'\n",
        "lat_name = None\n",
        "lon_name = None\n",
        "for name in (\"lat\", \"latitude\"):\n",
        "    if name in da.coords:\n",
        "        lat_name = name\n",
        "        break\n",
        "for name in (\"lon\", \"longitude\", \"lonc\"):\n",
        "    if name in da.coords:\n",
        "        lon_name = name\n",
        "        break\n",
        "\n",
        "if lat_name is None or lon_name is None:\n",
        "    raise RuntimeError(f\"Could not find lat/lon coordinate names. Coordinates available: {list(da.coords)}\")\n",
        "\n",
        "print(\"Using coordinates:\", lat_name, lon_name)\n",
        "\n",
        "# Convert DataArray into a long-form DataFrame [lon, lat, time, value]\n",
        "# We stack the spatial dims into a single index then reset index to columns\n",
        "# If time dimension exists, it will be preserved.\n",
        "\n",
        "# Make sure coordinates are 1D and sorted (xarray usually has them)\n",
        "da_stacked = da.stack(points=(lat_name, lon_name)).reset_index(\"points\")\n",
        "# The stack produced an index 'points' containing tuple (lat, lon) or indices; convert to coords\n",
        "# Simpler: convert full dataset to dataframe then reset index\n",
        "df = da.to_dataframe().reset_index()\n",
        "\n",
        "# Columns typically include lat_name, lon_name, maybe 'time' and the data variable column (same as variable name or first_var)\n",
        "# Normalize column names\n",
        "value_col = list(da.coords.keys())[0] if False else (variable if variable in da.coords else da.name)\n",
        "# Actually da.to_dataframe places the variable as a column with da.name\n",
        "# Find actual column representing values:\n",
        "possible_value_cols = [c for c in df.columns if c not in [lat_name, lon_name, \"time\", \"year\", \"points\"]]\n",
        "# choose the column that is numeric and not coordinate\n",
        "val_col = None\n",
        "for c in possible_value_cols:\n",
        "    if pd.api.types.is_numeric_dtype(df[c]):\n",
        "        val_col = c\n",
        "        break\n",
        "if val_col is None:\n",
        "    # fallback: the last column\n",
        "    val_col = df.columns[-1]\n",
        "\n",
        "print(\"Value column detected as:\", val_col)\n",
        "\n",
        "# If dataset uses 'year' instead of 'time', we keep whichever exists\n",
        "time_col = \"time\" if \"time\" in df.columns else (\"year\" if \"year\" in df.columns else None)\n",
        "\n",
        "# Create GeoDataFrame of grid points\n",
        "gdf_points = gpd.GeoDataFrame(\n",
        "    df,\n",
        "    geometry=gpd.points_from_xy(df[lon_name], df[lat_name]),\n",
        "    crs=\"EPSG:4326\"   # IMD lat/lon are geographic\n",
        ")\n",
        "\n",
        "# Read AOI shapefile and reproject to match points CRS (EPSG:4326)\n",
        "aoi = gpd.read_file(shapefile_path)\n",
        "if aoi.crs is None:\n",
        "    print(\"Warning: AOI shapefile has no CRS. Assuming EPSG:4326. If wrong, reproject your shapefile.\")\n",
        "    aoi = aoi.set_crs(\"EPSG:4326\")\n",
        "else:\n",
        "    aoi = aoi.to_crs(\"EPSG:4326\")\n",
        "\n",
        "# Option: dissolve all polygons into one (if you want a single AOI)\n",
        "# aoi_union = aoi.unary_union\n",
        "# If you want per-polygon outputs, keep original features\n",
        "\n",
        "# Spatial join: keep only grid points inside AOI polygons\n",
        "print(\"Filtering points inside AOI...\")\n",
        "# We'll spatial join points to AOI, producing polygon attribute columns (if any)\n",
        "joined = gpd.sjoin(gdf_points, aoi, how=\"inner\", predicate=\"within\")\n",
        "# joined now contains columns from both; for multi-polygons each point gets the polygon attributes.\n",
        "\n",
        "if joined.empty:\n",
        "    print(\"No grid points found inside AOI. Check shapefile and dataset extents.\")\n",
        "else:\n",
        "    # Keep relevant columns: time (if exists), lat, lon, value\n",
        "    cols_to_keep = [lon_name, lat_name, val_col]\n",
        "    if time_col:\n",
        "        cols_to_keep.insert(0, time_col)\n",
        "    # include AOI attributes for identification (like index_right or any ID)\n",
        "    # 'index_right' is added by sjoin: it links to AOI row index\n",
        "    if \"index_right\" in joined.columns:\n",
        "        cols_to_keep.append(\"index_right\")\n",
        "\n",
        "    export_df = joined[cols_to_keep].copy()\n",
        "    # rename columns to consistent names\n",
        "    rename_map = {lon_name: \"lon\", lat_name: \"lat\", val_col: variable}\n",
        "    if time_col:\n",
        "        rename_map[time_col] = \"time\"\n",
        "    export_df = export_df.rename(columns=rename_map)\n",
        "\n",
        "    # Save combined CSV\n",
        "    os.makedirs(os.path.dirname(out_csv) or \".\", exist_ok=True)\n",
        "    export_df.to_csv(out_csv, index=False)\n",
        "    print(\"Saved combined CSV:\", out_csv)\n",
        "\n",
        "    # Optionally save one CSV per polygon (index_right)\n",
        "    if save_one_csv_per_polygon:\n",
        "        for idx, grp in export_df.groupby(\"index_right\"):\n",
        "            poly_csv = os.path.join(one_csv_folder, f\"polygon_{idx}_{variable}_{start_yr}_{end_yr}.csv\")\n",
        "            grp.to_csv(poly_csv, index=False)\n",
        "        print(\"Saved per-polygon CSVs to:\", one_csv_folder)\n"
      ],
      "metadata": {
        "id": "dxoZ92AHvwNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide the alttitude & Longitude of a point for which the data is required\n",
        "#  And save the data into CSV file\n",
        "\n",
        "lat = 20.03 #lattitude of point\n",
        "lon = 77.23 #longitude of point\n",
        "data.to_csv('data.csv', lat, lon, path)"
      ],
      "metadata": {
        "id": "jMlaw45Dv5UE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save CSV files for multiple points\n",
        "\n",
        "# Provide lat and long in a list\n",
        "latLong = [[20.3,77.23],[23.5,72.5],[26.0,77,1]]\n",
        "\n",
        "for points in latLong:\n",
        "  lat = points[0]\n",
        "  lon = points[1]\n",
        "\n",
        "  data.to_csv('test.csv', lat, lon, path)\n",
        "  print (\"data save for \",points)"
      ],
      "metadata": {
        "id": "o5BHr3Fev3W6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Provide the Geojson file of a catchment or polygon to dowlnaod all the gridded data lying into that polygon\n",
        "\n",
        "geojson_file = '/content/drive/MyDrive/IMD/Test_geojson.geojson'\n",
        "\n",
        "\n",
        "url=\"https://drive.google.com/file/d/111XvmUzvTlhY2lbFMseGVhZQh4pisFXQ/view?usp=sharing\"\n",
        "url2='https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
        "\n",
        "points_df = pd.read_csv(url2)\n",
        "\n",
        "\n",
        "geometry = [Point(xy) for xy in zip(points_df['Long'], points_df['Lat'])]\n",
        "\n",
        "# Creating the GeoDataFrame\n",
        "gdf_points = gpd.GeoDataFrame(points_df, geometry=geometry)\n",
        "\n",
        "# Set a CRS (coordinate reference system), EPSG:4326 is WGS84 Lat/Long\n",
        "gdf_points.set_crs(epsg=4326, inplace=True)\n",
        "\n",
        "\n",
        "gdf_polygon = gpd.read_file(geojson_file)\n",
        "\n",
        "# Ensure both GeoDataFrames use the same CRS\n",
        "if gdf_points.crs != gdf_polygon.crs:\n",
        "    gdf_points = gdf_points.to_crs(gdf_polygon.crs)\n",
        "\n",
        "gdf_list = []\n",
        "for row in range (len(gdf_polygon)):\n",
        "    points_in_polygon = gdf_points[gdf_points.within(gdf_polygon.iloc[row].geometry)]\n",
        "    gdf_list.append(points_in_polygon)\n",
        "\n",
        "final_gdf = gpd.GeoDataFrame(pd.concat(gdf_list, ignore_index=True))\n",
        "\n",
        "final_df = final_gdf[[\"Name\",\"Lat\",\"Long\"]]\n",
        "final_df.to_csv(\"Master_file.csv\")\n",
        "\n",
        "for index, row in final_df.iterrows():\n",
        "    lat = row['Lat']\n",
        "    lon = row['Long']\n",
        "    data.to_csv('test.csv', lat, lon, path)\n",
        "    print (\"data save for \" + str(lat)+ \"_\" +str(lon))"
      ],
      "metadata": {
        "id": "A_tNtQFPv61j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}